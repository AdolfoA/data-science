{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data from Nested HTML Pages with Python Selenium \n",
    "In this tutorial, I illustrate how to scrape a list of terms, distributed over two levels of nested pages, through Python `selenium`.  As example, I scrape the list of terms from [Bocardi](https://www.brocardi.it).\n",
    "\n",
    "## Recognize the Web Site Structure\n",
    "In order to scrape data from a Web site, firstly I need to study the URIs structure. \n",
    "In my example, the list of terms is organized alphabetically, and for each letter of the alphabet there is a dedicated page, available at `<basic_url>/dizionario/<current_letter>/` (first level of URI). For example, for the letter `a`, the dedicated page is available at `https://www.brocardi.it/dizionario/a/`. \n",
    "In addition, the list of terms for each letter is paginated in different pages. For each letter, the first page is available the the first level of URI, while starting from the second page, the URI changes and is available at `<basic_url>/dizionario/<current_letter>/?page=<page_number>`. For example, for the letter `a`, the list of terms in the second page is available at the link `https://www.brocardi.it/dizionario/a/?page=2`.\n",
    "\n",
    "## Environment Setup\n",
    "In my code, I need to implement two loops: an external loop for letters and an internal loop for pages. I note that some letters are missing (`jkwxy`). For the external loop, I build a list containing all the letters, but the missing ones. I exploit `string.ascii_lowercase` to build the list of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "letters = string.ascii_lowercase\n",
    "letters = letters.replace('jk', '')\n",
    "letters = letters.replace('wxy', '')\n",
    "letters = list(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I define two variables, `basic_url`, which contains the basic url to the Web site, and `table`, which will contain the list of all extracted terms. Initially `table` is an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "basic_url = \"https://www.brocardi.it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I import all the `selenium` drivers and the `NoSuchElementException` exception, which will be used to catch some kind of exceptions, while performing the internal loop. I also import the `pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Loops\n",
    "I implement the external loop through a `for` ranging from `a` to `z`. At each step of the external loop, I build the url. Then I implement the internal infinite loop through a `while`. Within the internal loop I build a driver, which performs scraping. I exploit a `Chrome()` webdriver, which receives as input the `--headless` and the `--lang=it` options. The first options specifies that the browser will not be opened, while the second option specifies the language of the browser.\n",
    "\n",
    "Once connected, I search for two elements:\n",
    "* the elements which contain the list of terms\n",
    "* the element which contains the link to the next page.\n",
    "\n",
    "Both elements depend on the structure of the HTML page. I exploit the function `find_elements_by_xpath()` to search for a specific XPath.\n",
    "\n",
    "As already said, the internal loop is an infinite loop, where the break condition is given by a `NoSuchElementException`, raised when there are no further next pages. The list of terms is stored in the `table` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in letters:\n",
    "    \n",
    "    url = basic_url + '/dizionario/' + letter + '/'\n",
    "    while True:\n",
    "        try:\n",
    "            print(url)\n",
    "            options = Options()  \n",
    "            options.add_argument(\"--headless\") \n",
    "            options.add_argument(\"--lang=it\");\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            # get the list of terms\n",
    "            xpath = '//ul[@class=\"terms-list\"]'\n",
    "            words = driver.find_element_by_xpath(xpath).text\n",
    "            table.extend(words.split('\\n'))\n",
    "            \n",
    "            # get the next page\n",
    "            xpath = '//a[@class=\"next\"]'\n",
    "            url = driver.find_element_by_xpath(xpath).get_attribute('href')\n",
    "            \n",
    "            driver.close()\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results\n",
    "\n",
    "The variable `table` contains the list of all terms. I can store it to a CSV file. This can be done by building a `pandas` Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(table, columns=['word'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word'] = df['word'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('outputs/glossary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
